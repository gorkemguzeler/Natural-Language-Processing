{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JWvnWFHt_TqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2f7c70-30e9-4402-c945-4d80c83b83c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn<0.24\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.4.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.64.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.8 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install sklearn-crfsuite\n",
        "\n",
        "# YOU NEED TO RESTART THE RUNTIME!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZteNYk03GPL",
        "outputId": "42d388e3-3697-4cc7-8c07-805bff3265ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to mount your drive to this notebook in order to read the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Du79PuQ7-MD8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuZQf_I-PIq"
      },
      "source": [
        "## Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YF7EABYQ-JNt"
      },
      "outputs": [],
      "source": [
        "# Put the folder path where the datasets are located\n",
        "PATH = \"/content/drive/MyDrive/cs445/hw2/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fmYh526h-QC1"
      },
      "outputs": [],
      "source": [
        "# function to read data, return list of tuples each tuple represents a token contains word, pos tag, chunk tag, and ner tag\n",
        "def read_data(filename) -> list:\n",
        "\n",
        "  my_list = []\n",
        "  file1 = open(PATH + filename, 'r')\n",
        "  count = 0\n",
        "  find_sentence = []\n",
        "    \n",
        "  while True:\n",
        "\n",
        "      # Get next line from file\n",
        "      line = file1.readline()\n",
        "\n",
        "      #end of file is reached\n",
        "      if not line:\n",
        "          break\n",
        "\n",
        "      line=line.split()\n",
        "\n",
        "      if len(line) != 0:\n",
        "        if line[0]!= \"-DOCSTART-\" and line[1]!= \"-X-\":\n",
        "          temp = (line[0], line[1] ,line[2], line[3])\n",
        "          find_sentence.append(temp)\n",
        "        else: \n",
        "          line = file1.readline()\n",
        "\n",
        "      elif len(line) == 0: #if line is empty; advance to next sentence\n",
        "        my_list.append(find_sentence)\n",
        "        find_sentence = []\n",
        "    \n",
        "  return my_list\n",
        "\n",
        "# https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pgPCBhbj-dSJ"
      },
      "outputs": [],
      "source": [
        "# read data with your custom function\n",
        "train_data = read_data(\"/train.txt\")\n",
        "val_data = read_data(\"/valid.txt\")\n",
        "test_data = read_data(\"/test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOuOBvSp6d_3",
        "outputId": "d0373699-e582-41ab-d946-9784f5372564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14041 3250 3453\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data), len(val_data),len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECYsXDBl-7mx"
      },
      "source": [
        "# Create Gazetteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDXdXNV8Dsyy"
      },
      "outputs": [],
      "source": [
        "# load wikipedia pages\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "json_path = \"/content/drive/MyDrive/cs445/hw2/wikipedia_pages/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qer24PQU2UKQ"
      },
      "outputs": [],
      "source": [
        "# I saved the gazetteer to drive and do not run below part again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv1wP5bdo27s"
      },
      "outputs": [],
      "source": [
        "onlyfiles = [f for f in listdir(json_path) if isfile(join(json_path, f))]\n",
        "len(onlyfiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10yrXD7NORQi"
      },
      "outputs": [],
      "source": [
        "# Opening JSON file\n",
        "import re\n",
        "\n",
        "my_gazetteer = {}\n",
        "counter = 0\n",
        "\n",
        "for k in onlyfiles:\n",
        "  f = open(json_path + k)\n",
        "\n",
        "  # returns JSON object as a dictionary\n",
        "  data = json.load(f)\n",
        "    \n",
        "  # find interwiki links from the content of the pages  \n",
        "  urls = re.findall(r'\"&gt;([^\\&]+)', data[\"text\"])\n",
        "\n",
        "  for m in urls:\n",
        "        if m not in my_gazetteer:\n",
        "            my_gazetteer[m] = counter\n",
        "            counter = counter + 1\n",
        "  \n",
        "  #print(\"URL List\", urls)\n",
        "  #print(\"number of urls found:\", len(urls), \"check website via: \", data[\"url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJvJ5qYuMjTp"
      },
      "outputs": [],
      "source": [
        "my_gazetteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUl3eUZ1qFLn"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/cs445/hw2/saved_dictionary.pkl', 'wb') as f:\n",
        "    pickle.dump(my_gazetteer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiykApFc2QTL"
      },
      "outputs": [],
      "source": [
        "# Use the saved gazetteer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pUo2A6N0NwY"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/cs445/hw2/saved_dictionary.pkl', 'rb') as f:\n",
        "    loaded_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2r_EaYTVy5V"
      },
      "outputs": [],
      "source": [
        "len(loaded_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irJ3PAIDNWKs"
      },
      "outputs": [],
      "source": [
        "# you can also define rules to improve your gazetteer  \n",
        "#check if lowercase\n",
        "for k, v in list(loaded_dict.items()):\n",
        "    if k[0].islower():\n",
        "        del loaded_dict[k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg9chijniS4I"
      },
      "outputs": [],
      "source": [
        "# print the size of your gazetteer\n",
        "len(loaded_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlYGWXaDhFyn"
      },
      "outputs": [],
      "source": [
        "my_gazetteer = loaded_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hbhw-VC_Bni"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INZH7AlE_Fnw"
      },
      "source": [
        "## Conditional Random Fields (CRF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ac67uI_Ity"
      },
      "source": [
        "### Extract features for CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKh8kvu6_Dze",
        "outputId": "d884d7f6-1c05-46a9-812a-41763f1ddc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y_8lrN7Z_bHz"
      },
      "outputs": [],
      "source": [
        "# create a function to extract features for each token\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def convert_shape(my_str: str):\n",
        "    x = \"\"\n",
        "    for k in my_str:\n",
        "        if k.isdigit():\n",
        "            x = x + \"d\"\n",
        "        elif k.isalpha():\n",
        "            if k.isupper():\n",
        "                x = x + \"X\"\n",
        "            else:     \n",
        "                x = x + \"x\"\n",
        "        else: \n",
        "            x = x + k\n",
        "    return x\n",
        "\n",
        "def convert_short(my_str: str):\n",
        "    x = \"\"\n",
        "    for k in range(len(my_str)):\n",
        "        if k != len(my_str)-1:\n",
        "            if my_str[k] != my_str[k+1]:\n",
        "                x = x + my_str[k] \n",
        "            else: \n",
        "                pass\n",
        "    x = x + my_str[k] \n",
        "    return x\n",
        "\n",
        "\n",
        "def token2features(sentence: list, idx: int) -> dict:\n",
        "    word = sentence[idx][0]\n",
        "    postag = sentence[idx][1]\n",
        "    chunk_tag = sentence[idx][2]\n",
        "\n",
        "    features = {\n",
        "        'stem': ps.stem(word),\n",
        "        'postag': postag,\n",
        "        'chunk_tag': chunk_tag,\n",
        "        'BOS': False,\n",
        "        'EOS': False,\n",
        "        'word_starts_with_upper()': word[0].isupper(),\n",
        "        'word_shape': convert_shape(word), \n",
        "        'short_word_shape':convert_short(convert_shape(word)),  \n",
        "        'contains_number': any(char.isdigit() for char in word),\n",
        "        'contains_hyphen': \"-\" in word,\n",
        "        'upper_digit_dash': any(char.isdigit() for char in word) and \n",
        "        any(char.isupper() for char in word) and \"-\" in word,\n",
        "        'contains_prefix': word[:4],\n",
        "        'contains_suffix': word[-4:],\n",
        "        'word_isupper()': word.isupper(),       \n",
        "        'stopwords_contains()': word in stopwords.words('english'),\n",
        "        'gazetteer_contains()': word in my_gazetteer,\n",
        "\n",
        "        #additional features:\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        \n",
        "    }\n",
        "\n",
        "    if idx > 0:\n",
        "        word1 = sentence[idx-1][0]\n",
        "        postag1 = sentence[idx-1][1]\n",
        "        chunk_tag1 = sentence[idx-1][2]\n",
        "\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:chunk_tag1': chunk_tag1,\n",
        "           \n",
        "            '-1:neighbors': word1,\n",
        "            '-1:neighbors_short_shape': convert_short(convert_shape(word1)),\n",
        "            '-1:neighbors_shape': convert_shape(word1),\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if idx < len(sentence)-1:\n",
        "        word1 = sentence[idx+1][0]\n",
        "        postag1 = sentence[idx+1][1]\n",
        "        chunk_tag1 = sentence[idx+1][2]\n",
        "\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:chunk_tag1': chunk_tag1,\n",
        "            \n",
        "            '+1:neighbors': word1,\n",
        "            '+1:neighbors_short_shape': convert_short(convert_shape(word1)),\n",
        "            '+1:neighbors_shape': convert_shape(word1),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v57POpXU_bK8"
      },
      "outputs": [],
      "source": [
        "# define function to process each token given a sentence\n",
        "def sent2features(sentence: list) -> list:\n",
        "  return [token2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "# get named entity labels from the sentence\n",
        "def sent2labels(sentence: list) -> list:\n",
        "  return [label for token, postag, chunk, label in sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ta1NN3b_vkl"
      },
      "outputs": [],
      "source": [
        "# prepare inputs and labels\n",
        "train_sents = [sent2features(s) for s in train_data]\n",
        "val_sents = [sent2features(s) for s in val_data]\n",
        "test_sents = [sent2features(s) for s in test_data]\n",
        "\n",
        "train_labels = [sent2labels(s) for s in train_data]\n",
        "val_labels = [sent2labels(s) for s in val_data]\n",
        "test_labels = [sent2labels(s) for s in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw4Buzl3HRr9"
      },
      "outputs": [],
      "source": [
        "#train_sents[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_XPjjvLAWHV",
        "outputId": "c61a2c34-43bb-4d5c-b710-e77aacf18798"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=CRF(all_possible_transitions=True, keep_tempfiles=None,\n",
              "                           max_iterations=100),\n",
              "             param_grid={'algorithm': ['lbfgs', 'l2sgd']})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the hyperparameter space that will be scanned.\n",
        "parameters = {'algorithm':[\"lbfgs\",\"l2sgd\"]} #which algorithm works better\n",
        "\n",
        "# initialize GridSearchCV for CRF \n",
        "crf = sklearn_crfsuite.CRF(max_iterations=100,\n",
        "    all_possible_transitions=True)\n",
        "\n",
        "pipe  = GridSearchCV(crf, cv=5,param_grid=parameters)\n",
        "\n",
        "# fitting the model for grid search \n",
        "pipe.fit(train_sents,train_labels,X_dev=val_sents, y_dev=val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlvm45kcAtar",
        "outputId": "c96dfcfb-3bb3-400a-f3eb-07b3740a646c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'algorithm': 'l2sgd'}\n"
          ]
        }
      ],
      "source": [
        "# print best parameter after tuning \n",
        "print(pipe.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8dGA7_0A1hp",
        "outputId": "ed8a29f1-94f8-42b0-e5ee-5e56746b2dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CRF(algorithm='l2sgd', all_possible_transitions=True, keep_tempfiles=None,\n",
              "    max_iterations=100)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize and train a crf model with best hyper-parameters\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='l2sgd',\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(train_sents,train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgfYVqnBA1lA",
        "outputId": "d37397f9-c466-4e85-ddee-a5799bb37f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
            "F1: 0.8364542815134832\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC      0.863     0.880     0.871      1668\n",
            "       I-LOC      0.773     0.743     0.758       257\n",
            "      B-MISC      0.845     0.759     0.800       702\n",
            "      I-MISC      0.676     0.676     0.676       216\n",
            "       B-ORG      0.815     0.790     0.802      1661\n",
            "       I-ORG      0.698     0.824     0.756       835\n",
            "       B-PER      0.859     0.870     0.864      1617\n",
            "       I-PER      0.898     0.952     0.924      1156\n",
            "\n",
            "   micro avg      0.829     0.844     0.836      8112\n",
            "   macro avg      0.803     0.812     0.806      8112\n",
            "weighted avg      0.831     0.844     0.836      8112\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# calculate f1-score and classification report for test using sklearn_crfsuite.metrics class\n",
        "labels = list(crf.classes_)\n",
        "labels.remove('O')\n",
        "print(labels)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "y_pred = crf.predict(test_sents)\n",
        "\n",
        "print(\"F1:\", metrics.flat_f1_score(test_labels, y_pred,\n",
        "                      average='weighted', labels=labels))\n",
        "\n",
        "print(sklearn_crfsuite.metrics.flat_classification_report(\n",
        "    test_labels, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipFaQfhV-4LQ",
        "outputId": "45007082-600b-4419-a7ea-a8a80051c84d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14041"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6Dso9uR_TnC",
        "outputId": "7a082151-4905-416d-adb5-4c4b580317cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3250"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqCdjg67A1on"
      },
      "outputs": [],
      "source": [
        "# start from the stem of the token and add features one by one and train a new model with each feature that you add\n",
        "\n",
        "#features sorted based on assignment document\n",
        "list_features= [\n",
        "'stem',         \n",
        "'postag',\n",
        "'chunk_tag',\n",
        "'BOS',\n",
        "'EOS',\n",
        "'word_starts_with_upper()',\n",
        "'word_shape',\n",
        "'short_word_shape',\n",
        "'contains_number',\n",
        "'contains_hyphen',\n",
        "'upper_digit_dash',\n",
        "'contains_prefix',\n",
        "'contains_suffix',\n",
        "'word_isupper()',\n",
        "'stopwords_contains()',\n",
        "'word.isdigit()',\n",
        "'word.istitle()',\n",
        "'word.lower()',\n",
        "\n",
        "'+1:neighbors',\n",
        "'-1:neighbors',\n",
        "\n",
        "'+1:neighbors_shape',\n",
        "'-1:neighbors_shape',\n",
        "\n",
        "'+1:neighbors_short_shape',\n",
        "'-1:neighbors_short_shape',\n",
        "\n",
        "'+1:postag',\n",
        "'-1:postag',\n",
        "\n",
        "'+1:chunk_tag1',\n",
        "'-1:chunk_tag1',\n",
        "\n",
        "'+1:word.istitle()',\n",
        "'-1:word.istitle()',\n",
        "\n",
        "'+1:word.isupper()',\n",
        "'-1:word.isupper()',\n",
        "\n",
        "'+1:word.lower()',\n",
        "'-1:word.lower()',\n",
        "\n",
        "'gazetteer_contains()',\n",
        "] \n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='l2sgd',\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "precision_results = []\n",
        "recall_results = []\n",
        "f1_results = []\n",
        "classification_report_lists = []\n",
        "\n",
        "for g in range(len(list_features)):\n",
        "    temp_features = list_features[:g]\n",
        "    \n",
        "    c_train_sents = []\n",
        "\n",
        "    for i in train_sents: #each sentence\n",
        "        c_train_words = []\n",
        "        for s in i: #each word\n",
        "            new_dict = {}\n",
        "            for k, v in list(s.items()): #features of each word\n",
        "                if k in temp_features:\n",
        "                    new_dict[k] = v\n",
        "            c_train_words.append(new_dict)\n",
        "        c_train_sents.append(c_train_words)\n",
        "        \n",
        "    #should I apply feature subset function to test sents as well?\n",
        "\n",
        "    crf.fit(c_train_sents,train_labels)\n",
        "\n",
        "    y_pred = crf.predict(val_sents)\n",
        "\n",
        "    f1_results.append(metrics.flat_f1_score(val_labels, y_pred,\n",
        "                        average='weighted', labels=labels))\n",
        "    \n",
        "    recall_results.append(metrics.flat_recall_score(val_labels, y_pred,\n",
        "                        average='weighted', labels=labels))\n",
        "    \n",
        "    precision_results.append(metrics.flat_precision_score(val_labels, y_pred,\n",
        "                        average='weighted', labels=labels))\n",
        "\n",
        "    classification_report_lists.append(sklearn_crfsuite.metrics.flat_classification_report(\n",
        "        val_labels, y_pred, labels=sorted_labels, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pvv-tSIhksy"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/cs445/hw2/f1_results.pkl'\", \"wb\") as f:   #Pickling\n",
        "   pickle.dump(f1_results, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cs445/hw2/recall_results.pkl'\", \"wb\") as f:   #Pickling\n",
        "   pickle.dump(recall_results, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cs445/hw2/precision_results.pkl'\", \"wb\") as f:   #Pickling\n",
        "   pickle.dump(precision_results, f)\n",
        " \n",
        "with open(\"/content/drive/MyDrive/cs445/hw2/classification_report_lists.pkl'\", \"wb\") as f:   #Pickling\n",
        "   pickle.dump(classification_report_lists, f)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqaKdhxfNsCi"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/cs445/hw2/f1_results.pkl'\", \"rb\") as f:   #Pickling\n",
        "   f1_loaded = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cs445/hw2/recall_results.pkl'\", \"rb\") as f:   #Pickling\n",
        "   recall_loaded = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cs445/hw2/precision_results.pkl'\", \"rb\") as f:   #Pickling\n",
        "   precision_loaded = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cs445/hw2/classification_report_lists.pkl'\", \"rb\") as f:   #Pickling\n",
        "   classification_loaded = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuI50IUladm6",
        "outputId": "8d9e1cc6-5081-4771-e1d2-5b607a930d01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f1_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WZKXnFFhGt9",
        "outputId": "935ba1b3-9df4-4361-b35e-c671a553bdad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                f1    recall  precision\n",
            "stem                      0.010779  0.006277   0.056931\n",
            "postag                    0.684798  0.588516   0.826873\n",
            "chunk_tag                 0.782985  0.746251   0.831331\n",
            "BOS                       0.767779  0.752179   0.791860\n",
            "EOS                       0.772295  0.754272   0.799179\n",
            "word_starts_with_upper()  0.771120  0.755783   0.796799\n",
            "word_shape                0.795270  0.795885   0.799104\n",
            "short_word_shape          0.810482  0.807509   0.816249\n",
            "contains_number           0.806470  0.805765   0.809727\n",
            "contains_hyphen           0.803524  0.798791   0.810854\n",
            "upper_digit_dash          0.802260  0.801581   0.806701\n",
            "contains_prefix           0.806804  0.804138   0.814102\n",
            "contains_suffix           0.830438  0.827502   0.835824\n",
            "word_isupper()            0.850663  0.844938   0.857965\n",
            "stopwords_contains()      0.847702  0.843543   0.853329\n",
            "word.isdigit()            0.847628  0.841567   0.855144\n",
            "word.istitle()            0.848347  0.844822   0.853234\n",
            "word.lower()              0.850410  0.843543   0.859537\n",
            "+1:neighbors              0.858197  0.855167   0.862485\n",
            "-1:neighbors              0.868872  0.856329   0.883352\n",
            "+1:neighbors_shape        0.886437  0.874346   0.899718\n",
            "-1:neighbors_shape        0.892233  0.880623   0.905466\n",
            "+1:neighbors_short_shape  0.893173  0.884459   0.903381\n",
            "-1:neighbors_short_shape  0.893014  0.886551   0.900215\n",
            "+1:postag                 0.893885  0.887249   0.901548\n",
            "-1:postag                 0.898362  0.890503   0.907210\n",
            "+1:chunk_tag1             0.895446  0.887481   0.904880\n",
            "-1:chunk_tag1             0.896083  0.886435   0.907022\n",
            "+1:word.istitle()         0.895160  0.887714   0.903865\n",
            "-1:word.istitle()         0.893350  0.886202   0.901862\n",
            "+1:word.isupper()         0.897746  0.890387   0.906104\n",
            "-1:word.isupper()         0.892588  0.886435   0.900100\n",
            "+1:word.lower()           0.898048  0.891085   0.906305\n",
            "-1:word.lower()           0.894609  0.885970   0.904922\n",
            "gazetteer_contains()      0.894309  0.887132   0.902953\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "\n",
        "data = {\n",
        "  \"f1\": f1_results,\n",
        "  \"recall\": recall_results,\n",
        "  \"precision\": precision_results\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data, index = [list_features])\n",
        "\n",
        "print(df) \n",
        "# display the result table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvZ-Qr14BzZh",
        "outputId": "1dba33b1-c86a-49a3-cfee-a339c768c214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC      0.903     0.929     0.916      1837\n",
            "       I-LOC      0.899     0.868     0.883       257\n",
            "      B-MISC      0.922     0.854     0.886       922\n",
            "      I-MISC      0.877     0.723     0.792       346\n",
            "       B-ORG      0.887     0.831     0.858      1341\n",
            "       I-ORG      0.789     0.847     0.817       751\n",
            "       B-PER      0.921     0.905     0.913      1842\n",
            "       I-PER      0.954     0.954     0.954      1307\n",
            "\n",
            "   micro avg      0.902     0.887     0.895      8603\n",
            "   macro avg      0.894     0.864     0.877      8603\n",
            "weighted avg      0.903     0.887     0.894      8603\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display the classification report for the best model\n",
        "print(classification_loaded[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDvyOctS-urd"
      },
      "outputs": [],
      "source": [
        "#merge val and train set in order to use all data we have before testing!!!\n",
        "train_sents.extend(val_sents)\n",
        "train_labels.extend(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReI_5DwK-QI8",
        "outputId": "6b5c37f9-cd8f-48f0-dede-9ab8d304d545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8323907245627405\n",
            "0.8348126232741617\n",
            "0.8317356595515588\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC      0.869     0.882     0.876      1668\n",
            "       I-LOC      0.828     0.770     0.798       257\n",
            "      B-MISC      0.810     0.769     0.789       702\n",
            "      I-MISC      0.580     0.690     0.630       216\n",
            "       B-ORG      0.835     0.757     0.794      1661\n",
            "       I-ORG      0.730     0.782     0.755       835\n",
            "       B-PER      0.845     0.866     0.856      1617\n",
            "       I-PER      0.889     0.952     0.919      1156\n",
            "\n",
            "   micro avg      0.831     0.835     0.833      8112\n",
            "   macro avg      0.798     0.809     0.802      8112\n",
            "weighted avg      0.832     0.835     0.832      8112\n",
            "\n"
          ]
        }
      ],
      "source": [
        "crf.fit(c_train_sents,train_labels)\n",
        "\n",
        "y_pred = crf.predict(test_sents)\n",
        "\n",
        "print(metrics.flat_f1_score(test_labels, y_pred,\n",
        "                    average='weighted', labels=labels))\n",
        "\n",
        "print(metrics.flat_recall_score(test_labels, y_pred,\n",
        "                    average='weighted', labels=labels))\n",
        "\n",
        "print(metrics.flat_precision_score(test_labels, y_pred,\n",
        "                    average='weighted', labels=labels))\n",
        "\n",
        "print(sklearn_crfsuite.metrics.flat_classification_report(\n",
        "    test_labels, y_pred, labels=sorted_labels, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4DNBgU6B4Qs"
      },
      "source": [
        "## Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h6b7iz44B6kb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model, Input, Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, Input, Dropout, LSTM, TimeDistributed, Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfVSNob9Ub_8",
        "outputId": "32e81df1-3ec0-4644-9a87-8820f01e1dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=2863e760a79ed7cd738c2b80d41d1c998f55514b3a69a50ea07af979e78d0407\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ECHVqOmSSVK7"
      },
      "outputs": [],
      "source": [
        "train_labels = [sent2labels(s) for s in train_data]\n",
        "val_labels = [sent2labels(s) for s in val_data]\n",
        "test_labels = [sent2labels(s) for s in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46yqjBiB6sA",
        "outputId": "ec0f2457-3e1b-487b-cb7a-e5b1ce9db64e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# find unique labels and create dictionary to map each label to a unique integer value\n",
        "\n",
        "# function to get unique values\n",
        "def unique(list1):\n",
        "  \n",
        "    unique_dict = {}\n",
        "    # initialize a null list\n",
        "      \n",
        "    # traverse for all elements\n",
        "    for x in list1: \n",
        "        for k in x:\n",
        "        # check if exists in unique_list or not\n",
        "            if k not in unique_dict:\n",
        "                unique_dict[k] = 1\n",
        "    # print list\n",
        "    return unique_dict.keys()\n",
        "      \n",
        "    \n",
        "unique(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dIMalZCEOJug"
      },
      "outputs": [],
      "source": [
        "x = list(unique(train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ePpmJMy5K7-n"
      },
      "outputs": [],
      "source": [
        "n_tag = len(list(unique(train_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBJs7GgIpszg",
        "outputId": "3c6ae1cd-b6e0-4a19-ebc9-148b6b5c37cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km0WKutKuJId",
        "outputId": "ffd99982-0ec1-4369-9a9e-24813e3d593a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG',\n",
              "       'I-PER', 'O'], dtype='<U6')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ2IY7_EcWp5",
        "outputId": "cd6a2f94-1d3e-4106-9380-32467f54fe89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "le.transform(['O'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_9p6SsLSBU5",
        "outputId": "cd63c18d-a45b-4c1c-c181-f12f9768bf66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
              " ['B-PER', 'I-PER'],\n",
              " ['B-LOC', 'O']]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train_labels[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LpL96XYd3aYb"
      },
      "outputs": [],
      "source": [
        "before_padding_train_labels = train_labels.copy()\n",
        "before_padding_val_labels = val_labels.copy()\n",
        "before_padding_test_labels = test_labels.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RW5H5AhoJHHH"
      },
      "outputs": [],
      "source": [
        "for k in range(len(train_labels)):\n",
        "    train_labels[k] = list(le.transform(train_labels[k]))\n",
        "\n",
        "for k in range(len(val_labels)):\n",
        "    val_labels[k] = list(le.transform(val_labels[k]))\n",
        "\n",
        "for k in range(len(test_labels)):\n",
        "    test_labels[k] = list(le.transform(test_labels[k]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5L_dIy2P5fv",
        "outputId": "661f6143-a223-4651-efdc-c72c0b5c6b60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 8, 1, 8, 8, 8, 1, 8, 8], [3, 7], [0, 8]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_labels[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qGaCQeOkSZ1X"
      },
      "outputs": [],
      "source": [
        "def sent2words(sentence: list) -> list:\n",
        "  return [token for token, postag, chunk, label in sentence]\n",
        "\n",
        "def sent2postag(sentence: list) -> list:\n",
        "  return [postag for token, postag, chunk, label in sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nIktNH84oyDT"
      },
      "outputs": [],
      "source": [
        "train_sents = [sent2words(s) for s in train_data]\n",
        "val_sents = [sent2words(s) for s in val_data]\n",
        "test_sents = [sent2words(s) for s in test_data]\n",
        "\n",
        "p_train_sents = [sent2postag(s) for s in train_data]\n",
        "p_val_sents = [sent2postag(s) for s in val_data]\n",
        "p_test_sents = [sent2postag(s) for s in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVMM7jLV8ch7",
        "outputId": "9858429a-bacc-4110-9de2-7d5674443e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Create your own word embeddings from scratch and load a pretrained word embeddings\n",
        "\n",
        "# You can check https://radimrehurek.com/gensim/models/word2vec.html for training a word embeddings from scratch\n",
        "train_wv = Word2Vec(sentences = train_sents, size=20, window=5, min_count=1, workers=4) #multiclass word2vec\n",
        "\n",
        "# You can check https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html and https://github.com/RaRe-Technologies/gensim-data for loading pretrained word embeddings. \n",
        "api_wv = api.load('glove-twitter-25')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqAKUFrQVy5o",
        "outputId": "1a8003ed-c177-4f0f-8946-d20ed4f7e898"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 8, 1, 8, 8, 8, 1, 8, 8], [3, 7], [0, 8]]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "train_labels[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0On6FKKtwmtd",
        "outputId": "ca599e74-4d90-441b-8539-3318c5153d6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
              " ['Peter', 'Blackburn'],\n",
              " ['BRUSSELS', '1996-08-22']]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "train_sents[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ivyZwJxjaq",
        "outputId": "e38ef360-5ee1-413d-c20f-e0caad7d46c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#find average\n",
        "max = 0\n",
        "for i in test_sents:\n",
        "  if max < len(i):\n",
        "    max = len(i)\n",
        "max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KDTroJQhh3fH"
      },
      "outputs": [],
      "source": [
        "#train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPMw7vkw8ckn",
        "outputId": "1844859e-06f7-459d-e9cd-1550cf874843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  988 10950   204   628     6  3938   215  5773     2     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [  773  1871     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [  725   149     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    1   218   333    13    12    69    27  7517    23   204  3939     6\n",
            "   2403     6 10951   215  5773   405  3381  2008   518  1744  1872   647\n",
            "    308    40  7518     6  1631     2     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [  123    14  2990     6     1   218   299    14  2659   800  7519  7520\n",
            "     13    12    74  2403   275   913  7521    26   538   126   124   136\n",
            "    405     1  2404  3939    19 10952     2     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "# preprare your dataset for RNN classifier (you need to add padding to labels as well)\n",
        "\n",
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(train_sents)\n",
        "\n",
        "train_sents = tokenizer.texts_to_sequences(train_sents)\n",
        "val_sents = tokenizer.texts_to_sequences(val_sents)\n",
        "test_sents = tokenizer.texts_to_sequences(test_sents)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "maxlen = 124\n",
        "#maxlen = 80\n",
        "\n",
        "train_sents = pad_sequences(train_sents, padding='post', maxlen=maxlen)\n",
        "val_sents = pad_sequences(val_sents, padding='post', maxlen=maxlen)\n",
        "test_sents = pad_sequences(test_sents, padding='post', maxlen=maxlen)\n",
        "\n",
        "train_labels = pad_sequences(train_labels, padding='post', maxlen=maxlen, value=8)\n",
        "val_labels = pad_sequences(val_labels, padding='post', maxlen=maxlen, value=8)\n",
        "test_labels = pad_sequences(test_labels, padding='post', maxlen=maxlen, value=8)\n",
        "\n",
        "print(train_sents[:5]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU2Cv11igAsp",
        "outputId": "9ad95dcb-d5ef-4ae7-930e-eea9f2896402"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 8, 1, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
              "       [3, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
              "       [0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_labels[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVATWkigif_R",
        "outputId": "e9b0fd56-b0ea-4dfa-d28b-fbe67dfb2ce0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 124)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9h0pB4KJYMBR"
      },
      "outputs": [],
      "source": [
        "train_labels = [to_categorical(i, num_classes=n_tag) for i in train_labels]\n",
        "\n",
        "val_labels = [to_categorical(i, num_classes=n_tag) for i in val_labels]\n",
        "\n",
        "test_labels = [to_categorical(i, num_classes=n_tag) for i in test_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAXaUP7sp-WB",
        "outputId": "4d135ea8-bceb-4e67-ac95-fb9f34909fa9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "train_labels[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gadm56-GdYyc",
        "outputId": "7b589d82-4c40-47b4-9777-c68b704b6642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "n_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwTvddKDS-8r"
      },
      "source": [
        "#RANDOMLY INITILIAZED"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import accuracy_score\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "from tensorflow.keras import layers\n",
        "from keras import optimizers \n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "metadata": {
        "id": "13SRnYUOejtd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "h1rZ6jb18cpl"
      },
      "outputs": [],
      "source": [
        "# Create Embedding Matrices and Layers\n",
        "\n",
        "input_dim = vocab_size\n",
        "output_dim = 100\n",
        "\n",
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Embedding layer\n",
        "    #model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    # Add bidirectional LSTM\n",
        "    model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(32, activation=\"relu\")))\n",
        "    model.add(TimeDistributed(Dense(n_tag, activation=\"softmax\")))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    #optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "    #optimizer='rmsprop'\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BuI62mb65_-y"
      },
      "outputs": [],
      "source": [
        "# Create your models and train them\n",
        "\n",
        "def train_model(X_train, y_train, X_val ,y_val, model):\n",
        "    loss = []\n",
        "    for i in range(10):\n",
        "        # fit model for one epoch on this sequence\n",
        "        hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=64, verbose=1, epochs=1)\n",
        "        loss.append(hist.history['loss'][0])\n",
        "    return loss, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DbhQz268cs8",
        "outputId": "89344cb3-bd47-4ede-bc53-bfbc950e280c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 124, 256)         234496    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 124, 256)          0         \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 124, 32)          8224      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 124, 9)           297       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,344,017\n",
            "Trainable params: 2,344,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 14s 26ms/step - loss: 0.1720 - accuracy: 0.9752 - val_loss: 0.0903 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 5s 22ms/step - loss: 0.0556 - accuracy: 0.9831 - val_loss: 0.1095 - val_accuracy: 0.9796\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.1316 - val_accuracy: 0.9811\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.1475 - val_accuracy: 0.9827\n",
            "220/220 [==============================] - 5s 24ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.1558 - val_accuracy: 0.9838\n",
            "220/220 [==============================] - 5s 21ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.1609 - val_accuracy: 0.9843\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.1657 - val_accuracy: 0.9847\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.1696 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 5s 21ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.1742 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.1772 - val_accuracy: 0.9852\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROFfj_2eTc_E"
      },
      "source": [
        "#Word embeddings trained from scratch with gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eo3aMqrzTcRv"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#vocab_len = len(train_wv.wv.vocab) + 1\n",
        "\n",
        "train_wv.save(\"train_wv.wordvectors\")\n",
        "embedding_vector = KeyedVectors.load(\"train_wv.wordvectors\", mmap='r')\n",
        "\n",
        "embedding_matrix = np.zeros((input_dim, 20))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "      #print(embedding_vector[word].shape, word)\n",
        "      embedding_matrix[i-1] = embedding_vector[word]\n",
        "    except KeyError:\n",
        "      #print(\"key error\", i, word)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuYlkqnMU03Q",
        "outputId": "fe20ae5f-4a7a-4226-e7ff-3b2915127e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 124, 256)         152576    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 124, 256)          0         \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 124, 32)          8224      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 124, 9)           297       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 581,297\n",
            "Trainable params: 581,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 14s 34ms/step - loss: 0.1198 - accuracy: 0.9737 - val_loss: 0.1023 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0663 - accuracy: 0.9819 - val_loss: 0.1017 - val_accuracy: 0.9804\n",
            "220/220 [==============================] - 5s 23ms/step - loss: 0.0414 - accuracy: 0.9878 - val_loss: 0.1210 - val_accuracy: 0.9812\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.1382 - val_accuracy: 0.9828\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0210 - accuracy: 0.9937 - val_loss: 0.1493 - val_accuracy: 0.9838\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1530 - val_accuracy: 0.9842\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0125 - accuracy: 0.9966 - val_loss: 0.1539 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.1595 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.1585 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.1612 - val_accuracy: 0.9853\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kmFcclEZljs"
      },
      "source": [
        "#Pretrained word embeddings from gensim.api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "r1ag1R9hZhLM"
      },
      "outputs": [],
      "source": [
        "api_wv.save(\"api_wv.wordvectors\")\n",
        "embedding_vector = KeyedVectors.load(\"api_wv.wordvectors\", mmap='r')\n",
        "\n",
        "embedding_matrix2 = np.zeros((input_dim, 25))\n",
        "\n",
        "unknown_counter = 0\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "      #print(embedding_vector[word], word)\n",
        "      embedding_matrix2[i-1] = embedding_vector[word]\n",
        "    except KeyError:\n",
        "      #print(\"key error\", i, word)\n",
        "      unknown_counter = unknown_counter + 1\n",
        "\n",
        "#print(unknown_counter) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOIkZSegbo_x",
        "outputId": "e12735a9-a362-4362-8de5-284953cb22de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 124, 256)         157696    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 124, 256)          0         \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, 124, 32)          8224      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDis  (None, 124, 9)           297       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 691,467\n",
            "Trainable params: 691,467\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 10s 23ms/step - loss: 0.1229 - accuracy: 0.9745 - val_loss: 0.0958 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 5s 25ms/step - loss: 0.0664 - accuracy: 0.9816 - val_loss: 0.0980 - val_accuracy: 0.9799\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0497 - accuracy: 0.9854 - val_loss: 0.1053 - val_accuracy: 0.9806\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0379 - accuracy: 0.9883 - val_loss: 0.1137 - val_accuracy: 0.9812\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0295 - accuracy: 0.9909 - val_loss: 0.1201 - val_accuracy: 0.9829\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0240 - accuracy: 0.9927 - val_loss: 0.1277 - val_accuracy: 0.9834\n",
            "220/220 [==============================] - 7s 30ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.1300 - val_accuracy: 0.9838\n",
            "220/220 [==============================] - 5s 24ms/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.1337 - val_accuracy: 0.9843\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.1367 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.1416 - val_accuracy: 0.9848\n"
          ]
        }
      ],
      "source": [
        "loss , api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Kv-rwcFKU0G3"
      },
      "outputs": [],
      "source": [
        "# define a function to remove paddings and align labels and tokens\n",
        "def align_predictions(predictions:np.array, label_ids:np.array):\n",
        "  predictions = np.argmax(predictions, axis=-1) \n",
        "  label_ids = np.argmax(label_ids, axis=-1) \n",
        "\n",
        "  preds_updated = []\n",
        "  labels_updated = []  \n",
        "\n",
        "  for k in range(len(predictions)):\n",
        "    preds_updated.append(list(le.inverse_transform(predictions[k])))\n",
        "\n",
        "  for k in range(len(label_ids)):\n",
        "    labels_updated.append(list(le.inverse_transform(label_ids[k])))\n",
        "  \n",
        "  preds_padding_removed = []\n",
        "  for i in range(len(before_padding_test_labels)):\n",
        "    preds_padding_removed.append(preds_updated[i][:len(before_padding_test_labels[i])])\n",
        "\n",
        "  return preds_padding_removed, labels_updated "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RESULTS"
      ],
      "metadata": {
        "id": "twDybAYzg9ms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzEkCwJ58cvh",
        "outputId": "98599152-1c2a-423f-d8da-73d8b25590b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.41      0.41      1668\n",
            "        MISC       0.38      0.38      0.38       702\n",
            "         ORG       0.50      0.40      0.44      1661\n",
            "         PER       0.29      0.16      0.21      1617\n",
            "\n",
            "   micro avg       0.41      0.33      0.37      5648\n",
            "   macro avg       0.40      0.34      0.36      5648\n",
            "weighted avg       0.40      0.33      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your models with functions of seqeval library\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#RANDOMLY INITILIAZED FIRST MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfDwu1LrruW3",
        "outputId": "fef16aae-112d-4208-c79d-133c033ad5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.42      0.40      0.41      1668\n",
            "        MISC       0.44      0.38      0.41       702\n",
            "         ORG       0.51      0.45      0.48      1661\n",
            "         PER       0.30      0.16      0.20      1617\n",
            "\n",
            "   micro avg       0.43      0.34      0.38      5648\n",
            "   macro avg       0.42      0.35      0.37      5648\n",
            "weighted avg       0.41      0.34      0.37      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#WV FIRST MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di-HJek8B5H7",
        "outputId": "296c83f7-2186-4699-f7de-6b0878c4f02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.43      0.39      0.41      1668\n",
            "        MISC       0.36      0.32      0.34       702\n",
            "         ORG       0.53      0.41      0.46      1661\n",
            "         PER       0.26      0.16      0.20      1617\n",
            "\n",
            "   micro avg       0.41      0.32      0.36      5648\n",
            "   macro avg       0.40      0.32      0.35      5648\n",
            "weighted avg       0.41      0.32      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#API FIRST MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7geXWMW7jhQ9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    #model.add(LSTM(units=128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
        "    #model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(16, activation=\"relu\")))\n",
        "    model.add(TimeDistributed(Dense(n_tag, activation=\"softmax\")))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vcMc_koc0OR",
        "outputId": "6df4648d-7cdb-4ee5-a78d-eaec0ec9256b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 124, 256)         234496    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 124, 256)          0         \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, 124, 16)          4112      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  (None, 124, 9)           153       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,339,761\n",
            "Trainable params: 2,339,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 9s 24ms/step - loss: 0.2267 - accuracy: 0.9620 - val_loss: 0.0991 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0775 - accuracy: 0.9805 - val_loss: 0.0902 - val_accuracy: 0.9786\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0475 - accuracy: 0.9844 - val_loss: 0.1080 - val_accuracy: 0.9807\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.1193 - val_accuracy: 0.9817\n",
            "220/220 [==============================] - 5s 22ms/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.1305 - val_accuracy: 0.9831\n",
            "220/220 [==============================] - 5s 22ms/step - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.1350 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 5s 25ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.1390 - val_accuracy: 0.9847\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.1440 - val_accuracy: 0.9845\n",
            "220/220 [==============================] - 5s 21ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.1459 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 6s 26ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1505 - val_accuracy: 0.9851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.40      0.41      1668\n",
            "        MISC       0.37      0.35      0.36       702\n",
            "         ORG       0.50      0.42      0.46      1661\n",
            "         PER       0.30      0.16      0.21      1617\n",
            "\n",
            "   micro avg       0.41      0.33      0.37      5648\n",
            "   macro avg       0.40      0.33      0.36      5648\n",
            "weighted avg       0.40      0.33      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#RANDOMLY INITILIAZED SECOND MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEVhS1QVc0LI",
        "outputId": "a4f11cf4-765e-47c6-c579-d5957b8da559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 124, 256)         152576    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 124, 256)          0         \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, 124, 16)          4112      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 124, 9)           153       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 577,041\n",
            "Trainable params: 577,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 10s 29ms/step - loss: 0.1106 - accuracy: 0.9763 - val_loss: 0.1023 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 6s 27ms/step - loss: 0.0685 - accuracy: 0.9817 - val_loss: 0.0961 - val_accuracy: 0.9804\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0440 - accuracy: 0.9872 - val_loss: 0.1092 - val_accuracy: 0.9812\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 0.1213 - val_accuracy: 0.9823\n",
            "220/220 [==============================] - 6s 28ms/step - loss: 0.0238 - accuracy: 0.9928 - val_loss: 0.1318 - val_accuracy: 0.9835\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.1423 - val_accuracy: 0.9844\n",
            "220/220 [==============================] - 5s 22ms/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 0.1464 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0118 - accuracy: 0.9968 - val_loss: 0.1518 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 6s 29ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.1548 - val_accuracy: 0.9852\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.1576 - val_accuracy: 0.9851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.43      0.41      0.42      1668\n",
            "        MISC       0.37      0.37      0.37       702\n",
            "         ORG       0.52      0.46      0.49      1661\n",
            "         PER       0.29      0.17      0.21      1617\n",
            "\n",
            "   micro avg       0.42      0.35      0.38      5648\n",
            "   macro avg       0.40      0.35      0.37      5648\n",
            "weighted avg       0.41      0.35      0.37      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#WV SECOND MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVm3QmVCczlM",
        "outputId": "36dec691-c648-44af-d0e7-beeaf36651c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 124, 256)         157696    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 124, 256)          0         \n",
            "                                                                 \n",
            " time_distributed_10 (TimeDi  (None, 124, 16)          4112      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_11 (TimeDi  (None, 124, 9)           153       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 687,211\n",
            "Trainable params: 687,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 9s 26ms/step - loss: 0.1437 - accuracy: 0.9693 - val_loss: 0.0974 - val_accuracy: 0.9786\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0699 - accuracy: 0.9809 - val_loss: 0.0965 - val_accuracy: 0.9791\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.1038 - val_accuracy: 0.9802\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0441 - accuracy: 0.9867 - val_loss: 0.1079 - val_accuracy: 0.9806\n",
            "220/220 [==============================] - 5s 21ms/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.1142 - val_accuracy: 0.9813\n",
            "220/220 [==============================] - 6s 26ms/step - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.1207 - val_accuracy: 0.9827\n",
            "220/220 [==============================] - 5s 24ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.1257 - val_accuracy: 0.9834\n",
            "220/220 [==============================] - 5s 20ms/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 0.1312 - val_accuracy: 0.9839\n",
            "220/220 [==============================] - 4s 20ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1341 - val_accuracy: 0.9843\n",
            "220/220 [==============================] - 4s 19ms/step - loss: 0.0130 - accuracy: 0.9962 - val_loss: 0.1393 - val_accuracy: 0.9844\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.39      0.40      0.40      1668\n",
            "        MISC       0.33      0.32      0.33       702\n",
            "         ORG       0.55      0.40      0.47      1661\n",
            "         PER       0.26      0.15      0.19      1617\n",
            "\n",
            "   micro avg       0.40      0.32      0.36      5648\n",
            "   macro avg       0.39      0.32      0.35      5648\n",
            "weighted avg       0.40      0.32      0.35      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#API SECOND MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKw7ncLXAxSk"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(16, activation=\"relu\")))\n",
        "    model.add(TimeDistributed(Dense(n_tag, activation=\"softmax\")))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQgQlQI4AxO2",
        "outputId": "8c717070-2573-4062-f7e7-0f73928c6a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 124, 128)         84480     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 124, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_12 (TimeDi  (None, 124, 16)          2064      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_13 (TimeDi  (None, 124, 9)           153       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,187,697\n",
            "Trainable params: 2,187,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 7s 20ms/step - loss: 0.2151 - accuracy: 0.9720 - val_loss: 0.0889 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0598 - accuracy: 0.9819 - val_loss: 0.0993 - val_accuracy: 0.9789\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0428 - accuracy: 0.9855 - val_loss: 0.1165 - val_accuracy: 0.9799\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0333 - accuracy: 0.9894 - val_loss: 0.1330 - val_accuracy: 0.9815\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0253 - accuracy: 0.9924 - val_loss: 0.1389 - val_accuracy: 0.9828\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0186 - accuracy: 0.9946 - val_loss: 0.1482 - val_accuracy: 0.9839\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0134 - accuracy: 0.9962 - val_loss: 0.1542 - val_accuracy: 0.9845\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.1572 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 4s 18ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.1613 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.1649 - val_accuracy: 0.9850\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.43      0.42      0.43      1668\n",
            "        MISC       0.39      0.39      0.39       702\n",
            "         ORG       0.50      0.42      0.46      1661\n",
            "         PER       0.30      0.16      0.21      1617\n",
            "\n",
            "   micro avg       0.42      0.34      0.38      5648\n",
            "   macro avg       0.40      0.35      0.37      5648\n",
            "weighted avg       0.41      0.34      0.37      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#random_model 3 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsodvb5ZA6XH",
        "outputId": "026f6c0b-3bd0-4e85-ff5d-6b79c8133070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 124, 128)         43520     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 124, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_14 (TimeDi  (None, 124, 16)          2064      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_15 (TimeDi  (None, 124, 9)           153       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 465,937\n",
            "Trainable params: 465,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 8s 22ms/step - loss: 0.1118 - accuracy: 0.9789 - val_loss: 0.1037 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0672 - accuracy: 0.9810 - val_loss: 0.1036 - val_accuracy: 0.9806\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0449 - accuracy: 0.9861 - val_loss: 0.1195 - val_accuracy: 0.9814\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0317 - accuracy: 0.9902 - val_loss: 0.1278 - val_accuracy: 0.9824\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.1378 - val_accuracy: 0.9838\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0164 - accuracy: 0.9952 - val_loss: 0.1482 - val_accuracy: 0.9842\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0130 - accuracy: 0.9964 - val_loss: 0.1530 - val_accuracy: 0.9845\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.1582 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.1609 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.1640 - val_accuracy: 0.9851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.40      0.38      0.39      1668\n",
            "        MISC       0.39      0.36      0.38       702\n",
            "         ORG       0.51      0.42      0.46      1661\n",
            "         PER       0.30      0.16      0.21      1617\n",
            "\n",
            "   micro avg       0.42      0.33      0.37      5648\n",
            "   macro avg       0.40      0.33      0.36      5648\n",
            "weighted avg       0.40      0.33      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#wv_model 3 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knqQx2YLA6S6",
        "outputId": "5e3c61d1-9b76-441c-878a-09e940ed045b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 124, 128)         46080     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 124, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_16 (TimeDi  (None, 124, 16)          2064      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_17 (TimeDi  (None, 124, 9)           153       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 573,547\n",
            "Trainable params: 573,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 7s 18ms/step - loss: 0.1893 - accuracy: 0.9616 - val_loss: 0.0974 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0751 - accuracy: 0.9805 - val_loss: 0.0946 - val_accuracy: 0.9790\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0593 - accuracy: 0.9826 - val_loss: 0.0989 - val_accuracy: 0.9800\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0476 - accuracy: 0.9855 - val_loss: 0.1061 - val_accuracy: 0.9807\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0391 - accuracy: 0.9878 - val_loss: 0.1133 - val_accuracy: 0.9812\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0323 - accuracy: 0.9897 - val_loss: 0.1205 - val_accuracy: 0.9821\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 0.1280 - val_accuracy: 0.9828\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.1315 - val_accuracy: 0.9836\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.1362 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0171 - accuracy: 0.9950 - val_loss: 0.1404 - val_accuracy: 0.9841\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.37      0.39      1668\n",
            "        MISC       0.32      0.30      0.31       702\n",
            "         ORG       0.46      0.42      0.44      1661\n",
            "         PER       0.27      0.14      0.18      1617\n",
            "\n",
            "   micro avg       0.39      0.31      0.34      5648\n",
            "   macro avg       0.36      0.31      0.33      5648\n",
            "weighted avg       0.37      0.31      0.33      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#api_model 3 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aWa07bTAxHx"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    #model.add(LSTM(units=128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
        "    #model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(32, activation=\"relu\")))\n",
        "    model.add(TimeDistributed(Dense(n_tag, activation=\"softmax\")))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo1ImOq7BFgB",
        "outputId": "b2ba8643-fa7f-4859-c7c0-66b9192fdf10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 124, 128)         84480     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 124, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_18 (TimeDi  (None, 124, 32)          4128      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_19 (TimeDi  (None, 124, 9)           297       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,189,905\n",
            "Trainable params: 2,189,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 7s 20ms/step - loss: 0.1833 - accuracy: 0.9767 - val_loss: 0.0874 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0539 - accuracy: 0.9831 - val_loss: 0.1068 - val_accuracy: 0.9799\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0358 - accuracy: 0.9886 - val_loss: 0.1258 - val_accuracy: 0.9821\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0242 - accuracy: 0.9928 - val_loss: 0.1345 - val_accuracy: 0.9837\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.1435 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.1501 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.1562 - val_accuracy: 0.9852\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.1623 - val_accuracy: 0.9853\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.1664 - val_accuracy: 0.9854\n",
            "220/220 [==============================] - 4s 16ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.1711 - val_accuracy: 0.9855\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.43      0.41      0.42      1668\n",
            "        MISC       0.40      0.42      0.41       702\n",
            "         ORG       0.57      0.44      0.50      1661\n",
            "         PER       0.30      0.17      0.21      1617\n",
            "\n",
            "   micro avg       0.44      0.35      0.39      5648\n",
            "   macro avg       0.43      0.36      0.39      5648\n",
            "weighted avg       0.43      0.35      0.38      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#random_model 4 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT_Mtie2BFcQ",
        "outputId": "c769656c-2494-4b8b-b131-81373c0465c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 124, 128)         43520     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 124, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_20 (TimeDi  (None, 124, 32)          4128      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_21 (TimeDi  (None, 124, 9)           297       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 468,145\n",
            "Trainable params: 468,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 7s 18ms/step - loss: 0.1417 - accuracy: 0.9695 - val_loss: 0.1020 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0700 - accuracy: 0.9811 - val_loss: 0.0988 - val_accuracy: 0.9803\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0450 - accuracy: 0.9868 - val_loss: 0.1214 - val_accuracy: 0.9812\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 0.1396 - val_accuracy: 0.9822\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0244 - accuracy: 0.9926 - val_loss: 0.1516 - val_accuracy: 0.9837\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.1577 - val_accuracy: 0.9842\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 0.1614 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 3s 14ms/step - loss: 0.0124 - accuracy: 0.9967 - val_loss: 0.1649 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.1658 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.1704 - val_accuracy: 0.9851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.39      0.41      0.40      1668\n",
            "        MISC       0.39      0.33      0.36       702\n",
            "         ORG       0.55      0.44      0.49      1661\n",
            "         PER       0.30      0.16      0.21      1617\n",
            "\n",
            "   micro avg       0.42      0.34      0.38      5648\n",
            "   macro avg       0.41      0.34      0.37      5648\n",
            "weighted avg       0.41      0.34      0.37      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#wv_model 4 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3fHTdFJBFYY",
        "outputId": "bf0803d3-88b6-4b59-fd97-8c334da65f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 124, 128)         46080     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 124, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_22 (TimeDi  (None, 124, 32)          4128      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_23 (TimeDi  (None, 124, 9)           297       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 575,755\n",
            "Trainable params: 575,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 7s 18ms/step - loss: 0.1770 - accuracy: 0.9606 - val_loss: 0.0978 - val_accuracy: 0.9787\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0715 - accuracy: 0.9811 - val_loss: 0.0948 - val_accuracy: 0.9799\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0544 - accuracy: 0.9843 - val_loss: 0.0981 - val_accuracy: 0.9808\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0428 - accuracy: 0.9873 - val_loss: 0.1042 - val_accuracy: 0.9814\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.1122 - val_accuracy: 0.9825\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0271 - accuracy: 0.9918 - val_loss: 0.1205 - val_accuracy: 0.9834\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0222 - accuracy: 0.9934 - val_loss: 0.1238 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0186 - accuracy: 0.9945 - val_loss: 0.1301 - val_accuracy: 0.9844\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 0.1326 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 3s 15ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.1348 - val_accuracy: 0.9847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.42      0.39      0.40      1668\n",
            "        MISC       0.37      0.32      0.34       702\n",
            "         ORG       0.48      0.43      0.46      1661\n",
            "         PER       0.27      0.14      0.19      1617\n",
            "\n",
            "   micro avg       0.40      0.32      0.36      5648\n",
            "   macro avg       0.38      0.32      0.35      5648\n",
            "weighted avg       0.39      0.32      0.35      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#api_model 4 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8zxEYZE-Jvr"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    model.add(layers.SimpleRNN(128,return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "    #model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(n_tag, activation='softmax'))\n",
        "\n",
        "    #model.add(TimeDistributed(Dense(32, activation=\"relu\")))\n",
        "    #model.add(TimeDistributed(Dense(n_tag, activation=\"softmax\")))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOOx_QYa-Jph",
        "outputId": "98b61552-83a0-4f62-b96d-0eea98d7bd42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_17 (Embedding)    (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " simple_rnn_5 (SimpleRNN)    (None, 124, 128)          29312     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 124, 9)            1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,131,473\n",
            "Trainable params: 2,131,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 31s 133ms/step - loss: 0.1395 - accuracy: 0.9764 - val_loss: 0.0879 - val_accuracy: 0.9807\n",
            "220/220 [==============================] - 35s 160ms/step - loss: 0.0444 - accuracy: 0.9885 - val_loss: 0.0908 - val_accuracy: 0.9837\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0219 - accuracy: 0.9953 - val_loss: 0.1008 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 27s 121ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.1088 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.1186 - val_accuracy: 0.9853\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.1239 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 25s 111ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.1296 - val_accuracy: 0.9852\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1361 - val_accuracy: 0.9854\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.1405 - val_accuracy: 0.9852\n",
            "220/220 [==============================] - 26s 119ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1450 - val_accuracy: 0.9853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.40      0.41      0.40      1668\n",
            "        MISC       0.39      0.35      0.37       702\n",
            "         ORG       0.53      0.39      0.45      1661\n",
            "         PER       0.34      0.19      0.25      1617\n",
            "\n",
            "   micro avg       0.42      0.33      0.37      5648\n",
            "   macro avg       0.41      0.33      0.37      5648\n",
            "weighted avg       0.42      0.33      0.37      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#random_model 5 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aothlre-JfV",
        "outputId": "93c90016-be1e-41e5-9ca8-89d6a1b1f5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_18 (Embedding)    (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " simple_rnn_6 (SimpleRNN)    (None, 124, 128)          19072     \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 124, 9)            1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 440,433\n",
            "Trainable params: 440,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 113ms/step - loss: 0.0845 - accuracy: 0.9804 - val_loss: 0.0989 - val_accuracy: 0.9792\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0924 - val_accuracy: 0.9820\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0367 - accuracy: 0.9895 - val_loss: 0.0977 - val_accuracy: 0.9827\n",
            "220/220 [==============================] - 25s 111ms/step - loss: 0.0256 - accuracy: 0.9929 - val_loss: 0.1035 - val_accuracy: 0.9835\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0188 - accuracy: 0.9949 - val_loss: 0.1081 - val_accuracy: 0.9843\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0142 - accuracy: 0.9964 - val_loss: 0.1124 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.1152 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.1188 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.1232 - val_accuracy: 0.9852\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.1248 - val_accuracy: 0.9849\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.41      0.41      1668\n",
            "        MISC       0.37      0.37      0.37       702\n",
            "         ORG       0.49      0.39      0.44      1661\n",
            "         PER       0.30      0.17      0.21      1617\n",
            "\n",
            "   micro avg       0.41      0.33      0.36      5648\n",
            "   macro avg       0.39      0.33      0.36      5648\n",
            "weighted avg       0.40      0.33      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#wv_model 5 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff1ATpMA-JWF",
        "outputId": "eefb4f36-fe13-40bf-f97e-7702e1c0489a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_19 (Embedding)    (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " simple_rnn_7 (SimpleRNN)    (None, 124, 128)          19712     \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 124, 9)            1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 546,123\n",
            "Trainable params: 546,123\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 27s 117ms/step - loss: 0.1083 - accuracy: 0.9754 - val_loss: 0.0993 - val_accuracy: 0.9791\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0625 - accuracy: 0.9829 - val_loss: 0.0942 - val_accuracy: 0.9807\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0473 - accuracy: 0.9862 - val_loss: 0.0944 - val_accuracy: 0.9816\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0368 - accuracy: 0.9892 - val_loss: 0.0969 - val_accuracy: 0.9825\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.1006 - val_accuracy: 0.9831\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0244 - accuracy: 0.9930 - val_loss: 0.1042 - val_accuracy: 0.9837\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0204 - accuracy: 0.9942 - val_loss: 0.1076 - val_accuracy: 0.9838\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0172 - accuracy: 0.9951 - val_loss: 0.1119 - val_accuracy: 0.9843\n",
            "220/220 [==============================] - 27s 124ms/step - loss: 0.0147 - accuracy: 0.9958 - val_loss: 0.1148 - val_accuracy: 0.9844\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.1177 - val_accuracy: 0.9844\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.40      0.41      1668\n",
            "        MISC       0.35      0.31      0.33       702\n",
            "         ORG       0.44      0.37      0.40      1661\n",
            "         PER       0.23      0.15      0.18      1617\n",
            "\n",
            "   micro avg       0.37      0.31      0.34      5648\n",
            "   macro avg       0.36      0.31      0.33      5648\n",
            "weighted avg       0.36      0.31      0.33      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#API 5 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8am-GA1p-JO6"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    model.add(layers.SimpleRNN(128,return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(n_tag, activation='softmax'))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzCtVVNbAJdq",
        "outputId": "1b8d9e42-dbd6-4ac1-e16a-f80389097668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_20 (Embedding)    (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " simple_rnn_8 (SimpleRNN)    (None, 124, 128)          29312     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 124, 9)            153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,132,529\n",
            "Trainable params: 2,132,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 116ms/step - loss: 0.2101 - accuracy: 0.9576 - val_loss: 0.1266 - val_accuracy: 0.9742\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0549 - accuracy: 0.9858 - val_loss: 0.1051 - val_accuracy: 0.9818\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0245 - accuracy: 0.9936 - val_loss: 0.1195 - val_accuracy: 0.9843\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0138 - accuracy: 0.9967 - val_loss: 0.1288 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.1363 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.1440 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.1499 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.1547 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1618 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 26s 120ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.1658 - val_accuracy: 0.9849\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.39      0.40      1668\n",
            "        MISC       0.39      0.37      0.38       702\n",
            "         ORG       0.47      0.41      0.43      1661\n",
            "         PER       0.30      0.18      0.23      1617\n",
            "\n",
            "   micro avg       0.40      0.33      0.36      5648\n",
            "   macro avg       0.39      0.34      0.36      5648\n",
            "weighted avg       0.39      0.33      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#random_model 6 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smi9V-ZSALqP",
        "outputId": "68a1108e-a4fb-4f03-c99b-04fcd107d797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_21 (Embedding)    (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " simple_rnn_9 (SimpleRNN)    (None, 124, 128)          19072     \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 124, 9)            153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 441,489\n",
            "Trainable params: 441,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 113ms/step - loss: 0.1126 - accuracy: 0.9714 - val_loss: 0.1082 - val_accuracy: 0.9788\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0546 - accuracy: 0.9838 - val_loss: 0.1054 - val_accuracy: 0.9816\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0311 - accuracy: 0.9909 - val_loss: 0.1230 - val_accuracy: 0.9829\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.1359 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0151 - accuracy: 0.9960 - val_loss: 0.1479 - val_accuracy: 0.9845\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.1574 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.1623 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.1622 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.1748 - val_accuracy: 0.9851\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.1755 - val_accuracy: 0.9849\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.42      0.41      0.42      1668\n",
            "        MISC       0.35      0.37      0.36       702\n",
            "         ORG       0.48      0.40      0.43      1661\n",
            "         PER       0.28      0.16      0.20      1617\n",
            "\n",
            "   micro avg       0.40      0.33      0.36      5648\n",
            "   macro avg       0.38      0.33      0.35      5648\n",
            "weighted avg       0.39      0.33      0.35      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#wv_model 6 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxowYkRiALmF",
        "outputId": "ec9c1cc1-d39e-4c70-f077-ab4ac9f7bd6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_22 (Embedding)    (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " simple_rnn_10 (SimpleRNN)   (None, 124, 128)          19712     \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 124, 9)            153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 547,179\n",
            "Trainable params: 547,179\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 115ms/step - loss: 0.1298 - accuracy: 0.9712 - val_loss: 0.1022 - val_accuracy: 0.9790\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0622 - accuracy: 0.9826 - val_loss: 0.0993 - val_accuracy: 0.9801\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0474 - accuracy: 0.9858 - val_loss: 0.1026 - val_accuracy: 0.9806\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0367 - accuracy: 0.9889 - val_loss: 0.1084 - val_accuracy: 0.9818\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0291 - accuracy: 0.9913 - val_loss: 0.1143 - val_accuracy: 0.9827\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0237 - accuracy: 0.9929 - val_loss: 0.1188 - val_accuracy: 0.9833\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0193 - accuracy: 0.9944 - val_loss: 0.1276 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0161 - accuracy: 0.9953 - val_loss: 0.1308 - val_accuracy: 0.9842\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.1353 - val_accuracy: 0.9842\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 0.1451 - val_accuracy: 0.9847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.42      0.39      0.40      1668\n",
            "        MISC       0.36      0.29      0.32       702\n",
            "         ORG       0.46      0.37      0.41      1661\n",
            "         PER       0.28      0.15      0.20      1617\n",
            "\n",
            "   micro avg       0.39      0.30      0.34      5648\n",
            "   macro avg       0.38      0.30      0.33      5648\n",
            "weighted avg       0.38      0.30      0.33      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#API 6 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5aakVoplALhl"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    model.add(layers.SimpleRNN(64,return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(n_tag, activation='softmax'))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLAC4xxRAjH_",
        "outputId": "e4a68025-4b08-42bb-8925-be6e794377c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_23 (Embedding)    (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " simple_rnn_11 (SimpleRNN)   (None, 124, 128)          29312     \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 124, 9)            153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,132,529\n",
            "Trainable params: 2,132,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 27s 117ms/step - loss: 0.1232 - accuracy: 0.9804 - val_loss: 0.0959 - val_accuracy: 0.9804\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0391 - accuracy: 0.9882 - val_loss: 0.1133 - val_accuracy: 0.9822\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0213 - accuracy: 0.9943 - val_loss: 0.1287 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.1420 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.1588 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.1726 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 26s 117ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1835 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1950 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 25s 116ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.2022 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.2083 - val_accuracy: 0.9849\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.39      0.41      0.40      1668\n",
            "        MISC       0.33      0.36      0.34       702\n",
            "         ORG       0.50      0.36      0.42      1661\n",
            "         PER       0.30      0.18      0.22      1617\n",
            "\n",
            "   micro avg       0.39      0.32      0.35      5648\n",
            "   macro avg       0.38      0.33      0.35      5648\n",
            "weighted avg       0.39      0.32      0.35      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#random_model 7 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV9-nmYQAjED",
        "outputId": "fcdbb831-e9f0-459b-ace1-0ce0782efc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_24 (Embedding)    (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " simple_rnn_12 (SimpleRNN)   (None, 124, 128)          19072     \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 124, 9)            153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 441,489\n",
            "Trainable params: 441,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 113ms/step - loss: 0.1132 - accuracy: 0.9753 - val_loss: 0.1087 - val_accuracy: 0.9786\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0950 - val_accuracy: 0.9818\n",
            "220/220 [==============================] - 24s 110ms/step - loss: 0.0334 - accuracy: 0.9899 - val_loss: 0.1126 - val_accuracy: 0.9825\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0221 - accuracy: 0.9934 - val_loss: 0.1242 - val_accuracy: 0.9836\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0158 - accuracy: 0.9957 - val_loss: 0.1338 - val_accuracy: 0.9844\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.1408 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.1540 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 25s 111ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.1625 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.1598 - val_accuracy: 0.9849\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.1687 - val_accuracy: 0.9852\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.41      0.41      0.41      1668\n",
            "        MISC       0.38      0.36      0.37       702\n",
            "         ORG       0.55      0.39      0.46      1661\n",
            "         PER       0.31      0.17      0.22      1617\n",
            "\n",
            "   micro avg       0.42      0.33      0.37      5648\n",
            "   macro avg       0.41      0.34      0.37      5648\n",
            "weighted avg       0.42      0.33      0.37      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, wv_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = wv_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#wv_model 7 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7H0rzwzAjAr",
        "outputId": "c4227b20-1a62-42a3-d066-44bba2ddbf20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_25 (Embedding)    (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 124, 128)          19712     \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 124, 16)           2064      \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 124, 9)            153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 547,179\n",
            "Trainable params: 547,179\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 113ms/step - loss: 0.1249 - accuracy: 0.9793 - val_loss: 0.1013 - val_accuracy: 0.9789\n",
            "220/220 [==============================] - 24s 110ms/step - loss: 0.0630 - accuracy: 0.9827 - val_loss: 0.0979 - val_accuracy: 0.9806\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0482 - accuracy: 0.9859 - val_loss: 0.0988 - val_accuracy: 0.9811\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0380 - accuracy: 0.9887 - val_loss: 0.1042 - val_accuracy: 0.9820\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.1095 - val_accuracy: 0.9831\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.1153 - val_accuracy: 0.9836\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.1213 - val_accuracy: 0.9842\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0164 - accuracy: 0.9953 - val_loss: 0.1266 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0139 - accuracy: 0.9961 - val_loss: 0.1302 - val_accuracy: 0.9845\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.1344 - val_accuracy: 0.9848\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.39      0.40      0.39      1668\n",
            "        MISC       0.37      0.32      0.34       702\n",
            "         ORG       0.48      0.37      0.42      1661\n",
            "         PER       0.24      0.14      0.18      1617\n",
            "\n",
            "   micro avg       0.38      0.31      0.34      5648\n",
            "   macro avg       0.37      0.31      0.33      5648\n",
            "weighted avg       0.37      0.31      0.33      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, api_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = api_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))\n",
        "#API 7 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UYKMN6T3Ai9b"
      },
      "outputs": [],
      "source": [
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    model.add(layers.SimpleRNN(64,return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(n_tag, activation='softmax'))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlZmUcsZAi5z",
        "outputId": "588a9612-94ff-4dfa-f406-fb7662361482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, 124, 64)           10560     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 124, 32)           2080      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 124, 9)            297       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,113,937\n",
            "Trainable params: 2,113,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 114ms/step - loss: 0.1875 - accuracy: 0.9663 - val_loss: 0.0887 - val_accuracy: 0.9792\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0491 - accuracy: 0.9852 - val_loss: 0.1099 - val_accuracy: 0.9817\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0293 - accuracy: 0.9910 - val_loss: 0.1250 - val_accuracy: 0.9830\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0186 - accuracy: 0.9944 - val_loss: 0.1324 - val_accuracy: 0.9840\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0120 - accuracy: 0.9965 - val_loss: 0.1445 - val_accuracy: 0.9845\n",
            "220/220 [==============================] - 27s 121ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.1592 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.1617 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1702 - val_accuracy: 0.9850\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.1769 - val_accuracy: 0.9852\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1827 - val_accuracy: 0.9852\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.40      0.40      0.40      1668\n",
            "        MISC       0.37      0.37      0.37       702\n",
            "         ORG       0.51      0.39      0.44      1661\n",
            "         PER       0.31      0.18      0.23      1617\n",
            "\n",
            "   micro avg       0.41      0.33      0.36      5648\n",
            "   macro avg       0.40      0.34      0.36      5648\n",
            "weighted avg       0.40      0.33      0.36      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDezWeemD8I4",
        "outputId": "d2c994dd-e154-4909-d574-249ac08f47c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 124, 20)           420200    \n",
            "                                                                 \n",
            " simple_rnn_4 (SimpleRNN)    (None, 124, 64)           5440      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 124, 32)           2080      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 124, 9)            297       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428,017\n",
            "Trainable params: 428,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 26s 114ms/step - loss: 0.1259 - accuracy: 0.9716 - val_loss: 0.1125 - val_accuracy: 0.9788\n",
            "220/220 [==============================] - 24s 111ms/step - loss: 0.0675 - accuracy: 0.9818 - val_loss: 0.1004 - val_accuracy: 0.9806\n",
            "220/220 [==============================] - 24s 110ms/step - loss: 0.0437 - accuracy: 0.9869 - val_loss: 0.1146 - val_accuracy: 0.9821\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.0298 - accuracy: 0.9905 - val_loss: 0.1313 - val_accuracy: 0.9831\n",
            "220/220 [==============================] - 27s 122ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.1435 - val_accuracy: 0.9836\n",
            "220/220 [==============================] - 24s 109ms/step - loss: 0.0175 - accuracy: 0.9950 - val_loss: 0.1546 - val_accuracy: 0.9844\n",
            "220/220 [==============================] - 23s 107ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.1571 - val_accuracy: 0.9841\n",
            "220/220 [==============================] - 24s 109ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.1628 - val_accuracy: 0.9848\n",
            "220/220 [==============================] - 24s 110ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.1649 - val_accuracy: 0.9846\n",
            "220/220 [==============================] - 24s 109ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.1727 - val_accuracy: 0.9848\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.40      0.40      0.40      1668\n",
            "        MISC       0.38      0.35      0.36       702\n",
            "         ORG       0.46      0.39      0.42      1661\n",
            "         PER       0.29      0.16      0.21      1617\n",
            "\n",
            "   micro avg       0.39      0.32      0.35      5648\n",
            "   macro avg       0.38      0.32      0.35      5648\n",
            "weighted avg       0.38      0.32      0.35      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(1,embedding_matrix))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PmRHiplD8zH",
        "outputId": "d1846acb-69c0-47e5-c8fb-f831923a28d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 124, 25)           525250    \n",
            "                                                                 \n",
            " simple_rnn_5 (SimpleRNN)    (None, 124, 64)           5760      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 124, 64)           0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 124, 32)           2080      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 124, 9)            297       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,387\n",
            "Trainable params: 533,387\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "220/220 [==============================] - 27s 119ms/step - loss: 0.1363 - accuracy: 0.9760 - val_loss: 0.1032 - val_accuracy: 0.9789\n",
            "220/220 [==============================] - 25s 115ms/step - loss: 0.0717 - accuracy: 0.9810 - val_loss: 0.0987 - val_accuracy: 0.9797\n",
            "220/220 [==============================] - 25s 114ms/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.1012 - val_accuracy: 0.9805\n",
            "220/220 [==============================] - 27s 122ms/step - loss: 0.0471 - accuracy: 0.9857 - val_loss: 0.1064 - val_accuracy: 0.9809\n",
            "220/220 [==============================] - 26s 117ms/step - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.1121 - val_accuracy: 0.9814\n",
            "220/220 [==============================] - 26s 117ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.1160 - val_accuracy: 0.9822\n",
            "220/220 [==============================] - 25s 116ms/step - loss: 0.0288 - accuracy: 0.9910 - val_loss: 0.1218 - val_accuracy: 0.9830\n",
            "220/220 [==============================] - 27s 122ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.1260 - val_accuracy: 0.9835\n",
            "220/220 [==============================] - 25s 116ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.1320 - val_accuracy: 0.9839\n",
            "220/220 [==============================] - 25s 116ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.1363 - val_accuracy: 0.9841\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.37      0.37      0.37      1668\n",
            "        MISC       0.29      0.24      0.26       702\n",
            "         ORG       0.43      0.34      0.38      1661\n",
            "         PER       0.23      0.15      0.18      1617\n",
            "\n",
            "   micro avg       0.35      0.28      0.31      5648\n",
            "   macro avg       0.33      0.28      0.30      5648\n",
            "weighted avg       0.34      0.28      0.31      5648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss, random_model = train_model(train_sents, np.array(train_labels), val_sents ,np.array(val_labels) ,get_bilstm_lstm_model(2,embedding_matrix2))\n",
        "\n",
        "preds = random_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.concatenate((train_sents,val_sents),axis=0)\n",
        "y = np.concatenate((train_labels,val_labels),axis=0)"
      ],
      "metadata": {
        "id": "7SCtHXp42Axu"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = vocab_size\n",
        "output_dim = 100\n",
        "\n",
        "def get_bilstm_lstm_model(embedding_class, embedding_matrix): # embedding_class, embedding_matrix\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Embedding layer\n",
        "    #model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "\n",
        "    if embedding_class == 0:\n",
        "        model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=maxlen))\n",
        "    elif embedding_class == 1:\n",
        "        model.add(Embedding(input_dim, output_dim=20, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim, output_dim=25, input_length=maxlen, weights=[embedding_matrix], trainable=True))\n",
        "    \n",
        "    # Add bidirectional LSTM\n",
        "    model.add(Bidirectional(LSTM(512,return_sequences=True)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Add timeDistributed Layer\n",
        "    model.add(TimeDistributed(Dense(32, activation=\"relu\")))\n",
        "    model.add(TimeDistributed(Dense(n_tag, activation=\"softmax\")))\n",
        "\n",
        "    #Optimiser \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    #optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "    #optimizer='rmsprop'\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "zDT2Ptby0jVc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, model):\n",
        "    loss = []\n",
        "    for i in range(20):\n",
        "        # fit model for one epoch on this sequence\n",
        "        hist = model.fit(X_train, y_train, batch_size=64, verbose=1, epochs=1)\n",
        "        loss.append(hist.history['loss'][0])\n",
        "    return loss, model"
      ],
      "metadata": {
        "id": "Fs6qVVp70jaf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, best_model = train_model(x, np.array(y), get_bilstm_lstm_model(0,0))\n",
        "\n",
        "preds = best_model.predict(test_sents)\n",
        "preds_updated,labels_updated  = align_predictions(preds,np.array(test_labels))\n",
        "\n",
        "print(classification_report(before_padding_test_labels, preds_updated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxplkpAN08Rk",
        "outputId": "d348e7bc-e871-46da-e703-cc27cd68f9c7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 124, 100)          2101000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 124, 1024)        2510848   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 124, 1024)         0         \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 124, 32)          32800     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 124, 9)           297       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,644,945\n",
            "Trainable params: 4,644,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "271/271 [==============================] - 19s 56ms/step - loss: 0.1204 - accuracy: 0.9765\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0600 - accuracy: 0.9809\n",
            "271/271 [==============================] - 16s 57ms/step - loss: 0.0504 - accuracy: 0.9850\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0411 - accuracy: 0.9887\n",
            "271/271 [==============================] - 15s 56ms/step - loss: 0.0306 - accuracy: 0.9923\n",
            "271/271 [==============================] - 16s 57ms/step - loss: 0.0231 - accuracy: 0.9943\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0183 - accuracy: 0.9954\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0153 - accuracy: 0.9960\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0130 - accuracy: 0.9965\n",
            "271/271 [==============================] - 16s 58ms/step - loss: 0.0110 - accuracy: 0.9969\n",
            "271/271 [==============================] - 16s 58ms/step - loss: 0.0094 - accuracy: 0.9973\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0083 - accuracy: 0.9976\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0072 - accuracy: 0.9979\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0065 - accuracy: 0.9980\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0057 - accuracy: 0.9982\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0051 - accuracy: 0.9984\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0046 - accuracy: 0.9985\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0041 - accuracy: 0.9987\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0037 - accuracy: 0.9988\n",
            "271/271 [==============================] - 15s 57ms/step - loss: 0.0033 - accuracy: 0.9989\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.50      0.46      0.48      1668\n",
            "        MISC       0.47      0.40      0.44       702\n",
            "         ORG       0.56      0.43      0.49      1661\n",
            "         PER       0.42      0.27      0.33      1617\n",
            "\n",
            "   micro avg       0.49      0.39      0.44      5648\n",
            "   macro avg       0.49      0.39      0.43      5648\n",
            "weighted avg       0.49      0.39      0.43      5648\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDnYCXXEUG_g"
      },
      "source": [
        "## My Report\n",
        "\n",
        "# DATASET:\n",
        "Our dataset is a tuple of 4 features (token, postag, chunk tag, ner label).\n",
        "\n",
        "We create a few functions such as sent2words, sent2tokens, sent2features. So that, we can use them in order to prepare dataset for the crf model. \n",
        "\n",
        "# Create gazetteer:\n",
        "In order to use for our CRF model, we create gazetteer. We have wikipedia dataset. First, we read each wikipedia file one by one using json. Then, apply some regex functions in order to extract the parts that could be counted as named entities. Then, I applied a rule such as removing words that do not start with capital letter. Using all these words, I created a dictionary. The reason I implemented dictionary is it is efficient in terms of time complexity. Once we train crf model, we benefit using dict.   \n",
        "\n",
        "\n",
        "# CRF: \n",
        " \n",
        "*Extracting features:* This is one of the most important parts. Here we extract several features from the dataset. I have 35 features including neighbors features. First a few features definitely affected the performance of model more compared to features that we used later. Despite the improvement rate decreases after a few features, model became much more robust once we add each feature.    \n",
        "\n",
        "\n",
        "After creating the features, I applied grid search so that I can decide which hyper-parameters work best for the model. I concluded that best params are:\n",
        "\n",
        "\n",
        "```\n",
        "CRF(algorithm='l2sgd', all_possible_transitions=True, keep_tempfiles=None,\n",
        "    max_iterations=100)\n",
        "```\n",
        "\n",
        "Then, since it is asked: I observed how each feature changes performance of the model by adding each feature one by one. Results show that each feature affected positively and I sticked with the model contains all features. Then, I **combined my validation and train dataset** so that we can use all the data we have in order to predict test data. We obtained 0.89 F1 score in the validation set and 0.83 in the test set. Here are the results:\n",
        "\n",
        "\n",
        "```\n",
        "VALIDATION:\n",
        "\n",
        "         precision    recall  f1-score   support\n",
        "\n",
        "       B-LOC      0.903     0.929     0.916      1837\n",
        "       I-LOC      0.899     0.868     0.883       257\n",
        "      B-MISC      0.922     0.854     0.886       922\n",
        "      I-MISC      0.877     0.723     0.792       346\n",
        "       B-ORG      0.887     0.831     0.858      1341\n",
        "       I-ORG      0.789     0.847     0.817       751\n",
        "       B-PER      0.921     0.905     0.913      1842\n",
        "       I-PER      0.954     0.954     0.954      1307\n",
        "\n",
        "   micro avg      0.902     0.887     0.895      8603\n",
        "   macro avg      0.894     0.864     0.877      8603\n",
        "weighted avg      0.903     0.887     0.894      8603\n",
        "\n",
        "\n",
        "TEST:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "       B-LOC      0.869     0.882     0.876      1668\n",
        "       I-LOC      0.828     0.770     0.798       257\n",
        "      B-MISC      0.810     0.769     0.789       702\n",
        "      I-MISC      0.580     0.690     0.630       216\n",
        "       B-ORG      0.835     0.757     0.794      1661\n",
        "       I-ORG      0.730     0.782     0.755       835\n",
        "       B-PER      0.845     0.866     0.856      1617\n",
        "       I-PER      0.889     0.952     0.919      1156\n",
        "\n",
        "   micro avg      0.831     0.835     0.833      8112\n",
        "   macro avg      0.798     0.809     0.802      8112\n",
        "weighted avg      0.832     0.835     0.832      8112\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "# RNN: \n",
        "\n",
        "(similar to previous project's cnn part, hence I used some parts of my previous report and codes.)\n",
        "\n",
        "In the RNN part, we have tried 3 different embedding strategies: Randomly initialized word embeddings, Word embeddings trained from scratch with gensim, Pretrained word embeddings from gensim.api. In order to do so:\n",
        "For the Randomly initialized word embeddings, we set embedding dimension, vocabulary size and input length parameters in the embedding layer of our network. So that, weights in the embedding matrix will be initialized randomly.\n",
        "For the Word embeddings trained from scratch with gensim, we import our trained word2vec model using keyedvectors feature of gensim.models and form the embedding vector. Using the embedding vector, we create the embedding matrix which will be given as a parameter in the embedding layer of our network.\n",
        "For the Pretrained word embeddings from gensim.api, we import our pre-trained api model using keyedvectors feature of gensim.models and form the embedding vector: Using the embedding vector, we create the embedding matrix which will be given as a parameter in the embedding layer of our network.\n",
        "\n",
        "We prepared our dataset for RNN model by fitting our tokenizer to the train dataset. Then, we transform each text in texts to a sequence of integers using texts_to_sequences method. We set a maxlen parameter after finding out the maximum length of our dataset and we apply padding.\n",
        "\n",
        "After preparing the dataset, we continue by building our model:\n",
        "For the output layer, we set our activation function as softmax since our task is multiclass classification and we want our output between [0,8]. Then, we set loss function categorical_crossentropy for this task. We use adam optimizer since it is the best one for this task after trying other optimizers such as rmsprop.  \n",
        "\n",
        "We have built 8 different models for each (3) embedding strategy -> in total 24 models for the NER task. \n",
        "\n",
        "Trying dense layer with different neurons: we observe that using higher neurons in the layer increased the f1 score around 0.1.\n",
        "\n",
        "Replacing bilstm layer with simple RNN layer: we observe that bilstm is trained much faster compared to RNN layer. Bilstm layer outperformed the simple RNN layer overall.     \n",
        "\n",
        "Using randomly initialized embedding layer, we obtained around 38 percent f1 score in the test dataset. \n",
        "\n",
        "Using word embeddings trained from scratch with gensim, we obtained around 36 percent accuracy in the test set which is slightly better than randomly initiliazed word embedding results. \n",
        "\n",
        "Last but not least, we obtained around 33 percent accuracy in the test dataset using pretrained word embeddings from gensim.api. \n",
        "\n",
        "At the end, I trained the model with best parameters (bilstm - 64, dense - 32 ) and merged validation dataset with train dataset. Then, I received 0.40 F1 score. I wanted to try higher neurons for the lstm whether it could work better and give a try for 512 neurons in the bilstm layer and trained for 20 epochs. I was not able to train each model for 20 epochs since it would take so much time. So, I did the longest training for the last model. I obtained 0.43 F1 score which is significantly higher than my other models. \n",
        "\n",
        "Overall, we observed that CRF Model outperforms the results of RNN model significantly.\n",
        "\n",
        "References:\n",
        "\n",
        "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n",
        "https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Project 2 Notebook.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}